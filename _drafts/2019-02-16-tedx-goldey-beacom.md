---
layout: post
title:  "A Flick of the Wrist: Defining the Next Generation of Human-Computer Interaction"
date:   2019-02-16
---

The following is a transcript of a talk I did Tedx Goldey Beacom in January 2019. They filmed it, which you can watch [here](//todo).

it was the hardest thing i've done in my life so far

in one sense i hated it but in another i could see myself doing it again

https://docs.google.com/document/d/1B3jesJT8B79Y2LRKbgeG11fbsMSTiDfoSJYE_w_xa88/edit

---


For years, we've been enchanted by the idea of magic. The thought that someone, with the wave of a wand, snap of a finger, or some special words, can completely change the world around them in an instant is an idea that has captured minds throughout history.

Right now, we live in a world where we manipulate the entirety of human knowledge on screens barely bigger than credit cards. I’d be willing to bet that everyone in this room has a device in their pocket that has more computing power than the technology we used to send people to space. And relative to the rest of human history, it’s all happened in the blink of an eye. It’s really difficult to overstate how far we’ve already come, even within my lifetime.

With the advent of extended reality, machine learning, and other emerging technologies, the way we work with computers and each other is going to drastically evolve over the next several years. We are increasingly able to not only perceive digital worlds in three dimensions but interact with and be seen by them in return. This is in essence, what the field of human-computer interaction is developing.

Today, I’m going to show you some HCI research projects that are pushing the boundaries of technology. I invite you to look ahead at how they will fundamentally change how we interact with computers, information, and each other.

But first, take a moment to dream with me. It’s 20XX, a Tuesday.

Imagine being a kid taking chemistry for the first time. Remember learning molecular geometry? The thing where atoms are arranged in various 3D shapes like tetrahedra and trigonal pyramidals? You had to draw them out on paper, using nothing but a pencil and ruler to visualize these abstract shapes that make up fundamental pieces of our world.

Or, maybe you were a bit luckier and did an activity where you arranged playdough and toothpicks like I did in high school. They are really sad and droopy. Instead of that, kids in 20XX play with holograms, building out octahedra and seesaws with their bare hands in space. They can manipulate atomic bonds intuitively, playing with digital representations that function as we understand things to at an atomic level.

Imagine you're out hiking and see a gorgeous landscape of mountains. Inspired by their beauty, you whip out your sketchbook, but you don't exactly have an entire collection of paints on your person. But that doesn't matter, as you draw thin wobbly lines, it transforms into a picturesque landscape painting right before your eyes.

You get home from work and jump into a game that basically puts you into the matrix. As the lone hero, you stand alone in a hostile world. You’re swarmed by agents, and dodge bullets in slow-motion using your entire body.

Now, you want to tell me that all of these sound crazy, requiring tech we don’t have, right? But it turns out, ‘20XX’

is actually 2018. These are some of the things we did *last year*.

The chemistry application is Project Pupil at Carnegie Mellon. The painting? An application by Memo Akten. The slow-motion shooter? Super Hot, which you can literally go to a VR arcade to play *right now*. So, what are these things anyway? How are we doing this?

For the uninitiated, XR is used as an umbrella term to describe a continuum of combinations of real and virtual objects interacting in tandem. This includes technologies like virtual reality, where your entire environment is digital, augmented reality, where you overlay flat images onto the real world, and any dimension in between.

Maybe you’ve played with primitive augmented reality systems, like Pokemon Go, or are lucky enough to have tried virtual reality system sellers like Beat Saber. The one thing XR technologies have in common is they use computers to shape your perception. XR as a spectrum can put you in wholly new and different environments, or simply add information to the real world.

Machine learning is essentially using particular algorithms to teach computers how to solve problems. It’s used in all sorts of applications, from mastering Go and powering the brains of self-driving cars,

to generate cats from a handful of lines. I drew that last one, he’s probably okay. There’s a lot of exciting work using machine learning to see the world through a computer’s eyes.

We're able to take artificial intelligence and show it parts of the world. We can show them our bodies, our paintings, how objects interact, see what they come up with, and use that to shape our perception. Machine learning is able to make sense of the vast amount of information in reality, while XR will help us see it more clearly. I feel like some of the most exciting developments have been through open source and publicly funded research projects.

OpenPose is a research project at Carnegie Mellon that uses machine learning to detect bodies in single images. It’s been used as the backbone for other work, including research projects that help put your whole body in virtual reality, and help you, or at least, a video of you, do intricate ballet dances.

Pix2Pix is a project at Berkeley that uses neural networks to generate images based on training data. It’s been further remixed into applications that turn your webcam feed into flowers, or turn photos of Wilmington’s skyline into gorgeous paintings that emulate Van Goh.

Project North Star is an augmented reality headset that you can literally 3D print anywhere in the world. There’s a community growing around sourcing and building these headsets, and I think we’ll see some interesting applications as it becomes more accessible. These are all open source, so anyone can take their work and build on top of it to make all sorts of applications, which they have.

“They” includes me. I’m currently building my own North Star. Some of the parts I was able to 3D print back at the University of Delaware, others were sourced from community members that have cropped up around the project. This happened over UD’s summer scholars program, where I took 10 weeks to learn the basics of XR development. After the semester started, I turned that experience into an undergraduate research project focused on getting cross-disciplinary students together to develop XR applications.

Just last week I went to Reality Virtually, a hackathon at MIT’s Media lab. I got together with over 400 other developers, artists, designers, and coders to make XR applications. The one rule for all projects at the hackathon was that they had to be open source, so that anyone around the world could take what they made and create new and interesting applications. Together, we made just under 100 XR projects including tools for physical therapy and accessibility, but also games and interactive art. My team made a VR escape room in under 5 days, and my advisor Dr. Barmaki’s physical therapy project won “Best VR application”.

In my mind, this technology really comes together in the concept of Mirrorworlds. Rather than ever leaving your physical space, this technology will help transform it around you into another parallel dimension. Chairs become mountains, walls become sunsets, and "the floor is lava" transforms from a simple kid's game into a visceral experience. You can interact with digital objects the same way as you would with physical, and interact with physical ones to an even greater effect. Your environment can show you how it works, as items show you how to use them. A guitar could teach you how to play itself, showing you where best to hold it to play particular chords. Or objects could change altogether, as tables turn into touch screens and pencils into wands.

The question is no longer “how can we make this work?” but rather “how should this feel?” We’re at point in history where what would have been considered “magic” is real. It’s here, and it’s now. And so, I leave you with this: What will you do with it? Thank you.
------

Slides Draft:
https://www.dropbox.com/s/mn222oa8jnyemju/A%20Flick%20of%20the%20Wrist%20Slides.pptx?dl=0

[project-pupil]: https://twitter.com/YujinAriza/status/1068619034827083783
[alphago]: https://www.alphagomovie.com/images/gallery/gallery-9.jpg
[pat]: "the use of math and code to find underlying patterns in data to solve problems that traditional programming approaches cannot"
[cat]: some of mom's cats
[dog]: diesel recognized by a neural net as, in fact, a dog
[pets]: all the pets
[mixed-reality-spectrum]: https://docs.microsoft.com/en-us/windows/mixed-reality/images/mixed-reality-spectrum-550px.png
[Intel]: https://youtu.be/VSHDyUXSNqY?t=1069
[sad-toothpick]: https://photos.app.goo.gl/d4FFiHeLDzRsa3916
[matrix]: https://youtu.be/xZ0OUq_kDh8?t=20
[super-hot]: https://www.youtube.com/watch?v=pzG7Wc6mbwE
[leap-light]: https://twitter.com/keiichiban/status/1034475041650630656
[mirror]: http://blog.leapmotion.com/mirrorworlds/
[david]: “It’s time to shift the conversation from what an AR system should look like, to what an AR experience should feel like.” - DAVID HOLZ co-founder and chief technology officer at Leap Motion.
[magic]: Any sufficiently advanced technology is indistinguishable from magic.
[painting]: https://vimeo.com/302624466
[corning]: youtu.be/jZkHpNnXLB0

“What is the Next Generation of Human-Computer Interaction?” CHI 2006 Workshop Proceedings, cs.tufts.edu/~jacob/workshop/report.pdf.
“Entering the Metaverse.” Liv Erickson, livi.link/entering-metaverse-pdf.
“The Chess Master and the Computer” Garry Kasparov marom.net.technion.ac.il/files/2016/07/Kasparov-2010.pdf
“How combined human and computer intelligence will redefine jobs” Brad Bush techcrunch.com/2016/11/01/how-combined-human-and-computer-intelligence-will-redefine-jobs/
https://theinternettimessupplement.wordpress.com/2013/03/15/st-peters-square-2005-v-2013/
https://courses.lumenlearning.com/boundless-chemistry/chapter/molecular-geometry/
Works Cited
Music Everywhere, www.etc.cmu.edu/projects/pupil/.
“AlphaGo.” Wikipedia, Wikimedia Foundation, 18 Dec. 2018, en.wikipedia.org/wiki/AlphaGo.
Ariza, Yujin. “More Players? Why Not Pic.twitter.com/WcrF5w30hV.” Twitter, Twitter, 30 Nov. 2018, twitter.com/YujinAriza/status/1068619034827083783.
“Badass Fingersnap.” TV Tropes, tvtropes.org/pmwiki/pmwiki.php/Main/BadassFingerSnap.
“Building a Gesture Recognition System Using Deep Learning - Joanna Materzyńska.” YouTube, YouTube, 13 Nov. 2017, youtu.be/keffWSqi67w.
“But for Me, It Was Tuesday.” TV Tropes, tvtropes.org/pmwiki/pmwiki.php/Main/ButForMeItWasTuesday.
CMU-Perceptual-Computing-Lab. “CMU-Perceptual-Computing-Lab/Openpose.” GitHub, 8 Jan. 2019, github.com/CMU-Perceptual-Computing-Lab/openpose.
“Clarke's Three Laws.” Wikipedia, Wikimedia Foundation, 26 Dec. 2018, en.wikipedia.org/wiki/Clarke's_three_laws.
“A Day Made of Glass 2: Same Day. Expanded Corning Vision (2012).” YouTube, YouTube, 3 Feb. 2012, youtu.be/jZkHpNnXLB0.
“A Day Made of Glass... Made Possible by Corning. (2011).” YouTube, YouTube, 7 Feb. 2011, youtu.be/6Cf7IL_eZ38.
“Deep Learning for VR/AR: Body Tracking.” Intel RealSense, realsense.intel.com/deep-learning-for-vr-ar/.
“Deep Learning for VR/AR: Body Tracking with Intel RealSense Technology.” YouTube, YouTube, 29 Nov. 2018, youtu.be/VSHDyUXSNqY.
“DensePose.” DensePose, densepose.org/.
“The End of Cloud Computing.” YouTube, YouTube, 15 July 2017, youtu.be/4QTAtFaIiyc.
“Fei-Fei Li on AI and Machine Learning.” YouTube, YouTube, 5 Feb. 2018, youtu.be/XlnbNFW2tX8.
“Full-Contact Magic.” TV Tropes, tvtropes.org/pmwiki/pmwiki.php/Main/FullContactMagic.
“Google Developer Day at GDC 2018 Livestream.” YouTube, YouTube, 19 Mar. 2018, youtu.be/5wtlj_q3DjE.
“How to Write a Conference Talk - The PL Enthusiast.” The Programming Languages Enthusiast, 2 Jan. 2019, www.pl-enthusiast.net/2019/01/02/how-to-write-a-conference-talk/.
Jezra. “How to Create Your TED Talk: An 8-Step Process.” Speak Up For Success, 10 Oct. 2018, speakupforsuccess.com/create-a-ted-talk/.
Kipman, Alex. “A Futuristic Vision of the Age of Holograms.” Ted, Ted, www.ted.com/talks/alex_kipman_the_dawn_of_the_age_of_holograms.
“Learning to See.” Memo Akten, www.memo.tv/portfolio/learning-to-see/.
“Learning to See: Gloomy Sunday.” Vimeo, 12 Jan. 2019, vimeo.com/260612034.
“Learning to See: We Are Made of Star Dust (#2).” Vimeo, 12 Jan. 2019, vimeo.com/242498070.
Li, Fei-Fei. “How We're Teaching Computers to Understand Pictures.” Ted, Ted, www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures.
“Magical Gesture.” TV Tropes, tvtropes.org/pmwiki/pmwiki.php/Main/MagicalGesture.
Matsuda, Keiichi. “HYPER-REALITY.” YouTube, YouTube, 19 May 2016, www.youtube.com/watch?v=YJg02ivYzSs.
Matsuda, Keiichi. “Using #ProjectNorthStar to Tap Back in to Physical Reality. Lots of Possibilities Spring from Combining #AR with #IoT. @Tweethue Pic.twitter.com/triANhovp7.” Twitter, Twitter, 28 Aug. 2018, twitter.com/keiichiban/status/1034475041650630656.
Matsuda, Keiichi. “Using #ProjectNorthStar to Tap Back in to Physical Reality. Lots of Possibilities Spring from Combining #AR with #IoT. @Tweethue Pic.twitter.com/triANhovp7.” Twitter, Twitter, 28 Aug. 2018, twitter.com/keiichiban/status/1034475041650630656.
“Maureen Fan Explains the Power of VR and Animation.” YouTube, YouTube, 5 Feb. 2018, youtu.be/1xVyQhthH3s.
“Mirrorworlds.” Leap Motion Blog, 16 July 2018, blog.leapmotion.com/mirrorworlds/.
Motion, Leap. “‘It's Weird That a Child with a Piece of Clay Has More Power than a Professional with a Computer." This Week, Leap Motion Was Featured in the @WSJ as Part of a Larger Conversation around Interfaces of the Future. Https://T.co/DCde8I0MwI.” Twitter, Twitter, 4 Oct. 2018, twitter.com/LeapMotion/status/1047833574689452034.
“The Next Leap: How A.I. Will Change the 3D Industry - Andrew Price.” YouTube, YouTube, 5 Nov. 2018, youtu.be/FlgLxSLsYWQ.
“Ohayo (オハヨウ) Satoshi Kon.” YouTube, YouTube, 5 Dec. 2010, youtu.be/qYUFBnAmK28.
“Origami: ReImagining Reality.” Procedural Worlds, 21 Mar. 2018, www.procedural-worlds.com/blog/origami-reimagining-reality/.
“The Potential of Shared Virtual Reality | Bruce Wooden | TEDxGeneva.” YouTube, YouTube, 16 June 2016, youtu.be/7mL0E4ykS7E.
“Project North Star.” Leap Motion Developer, developer.leapmotion.com/northstar/.
“Project North Star Is Now Open Source.” Leap Motion Blog, 6 June 2018, blog.leapmotion.com/north-star-open-source/.
“Project North Star: Exploring Augmented Reality.” YouTube, YouTube, 11 Apr. 2018, youtu.be/7m6J8W6Ib4w.
Raskin, Andy. “The Greatest Sales Deck I've Ever Seen – The Mission – Medium.” Medium.com, Medium, 15 Sept. 2016, medium.com/the-mission/the-greatest-sales-deck-ive-ever-seen-4f4ef3391ba0.
Raskin, Andy. “Want a Better Pitch? Watch This. – Firm Narrative – Medium.” Medium.com, Medium, 13 July 2015, medium.com/firm-narrative/want-a-better-pitch-watch-this-328b95c2fd0b.
“Real-World Games  |  Google Maps Platform  |  Google Cloud.” Google, Google, cloud.google.com/maps-platform/gaming/.
SUPERHOT. “SUPERHOT VR Release Trailer.” YouTube, YouTube, 5 Dec. 2016, www.youtube.com/watch?v=pzG7Wc6mbwE.
“Sorcerer's Apprentice - Fantasia.” Disney Video, video.disney.com/watch/sorcerer-s-apprentice-fantasia-4ea9ebc01a74ea59a5867853.
Stillman, Jessica. “5 Steps to Making Pitches Like Elon Musk.” Inc.com, Inc., 22 July 2015, www.inc.com/jessica-stillman/5-steps-to-pitch-like-elon-musk.html.
“Summoning and Superpowers: Designing VR Interactions at a Distance.” Leap Motion Blog, 25 Jan. 2018, blog.leapmotion.com/summoning-superpowers-designing-vr-interactions-distance/.
“Technology.” Leap Motion, www.leapmotion.com/technology/.
“Unveiling Project North Star.” Leap Motion Blog, 28 June 2018, blog.leapmotion.com/northstar/.
